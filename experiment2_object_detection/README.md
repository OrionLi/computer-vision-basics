# Experiment 2: Object Detection

Student: 李弢阳
ID: 202211621213

This experiment focuses on object detection using pre-trained Convolutional Neural Networks (CNNs) with PyTorch. The goal is to take an image, detect objects within it, and draw bounding boxes around them with appropriate labels.

## Directory Structure
- `data/`: This directory can be used to store sample images for detection. You will need to add your own images here.
- `src/`: Contains the Python scripts for the experiment.
  - `object_detector.py`: This script loads a pre-trained model (Faster R-CNN ResNet50 FPN V2), performs detection on a sample image (downloading one if necessary), filters results by confidence, draws bounding boxes and labels on the image, and saves the resulting image to the `output/` directory. It also prints detected object details to the console.
- `output/`: This directory will store the images with detected objects and their bounding boxes, generated by `object_detector.py`.

## Setup

### PyTorch Installation
Ensure you have PyTorch and Torchvision installed. The specified installation command is:
\`\`\`bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
\`\`\`
(Note: `cu128` implies a CUDA 12.8 compatible build. Adjust if using CPU or a different CUDA version. The object detection script will attempt to use CUDA if available, otherwise CPU.)

### Additional Libraries
You might need OpenCV for image manipulation (reading images, drawing bounding boxes, saving results) and Matplotlib for displaying images (optional, mainly for interactive sessions).
\`\`\`bash
pip3 install opencv-python matplotlib
\`\`\`

## Experiment Overview
1.  **Model Selection**: We will use a pre-trained object detection model from `torchvision.models.detection`, such as Faster R-CNN with a ResNet50 backbone. These models are trained on large datasets like COCO and can detect a variety of common objects.
2.  **Data (Sample Images)**: You will provide your own sample images for the model to perform detections on. Place these images in the `experiment2_object_detection/data/` directory.
3.  **Detection Script (`object_detector.py`)**:
    *   Load the pre-trained model.
    *   Load a sample image from the `data/` directory.
    *   Preprocess the image to the format expected by the model.
    *   Perform inference to get object detections (bounding boxes, class labels, and confidence scores).
    *   Filter detections based on a confidence threshold.
    *   Draw the bounding boxes and labels on the image.
    *   Save the resulting image to the `output/` directory (e.g., `output/detected_sample_image.jpg`).
4.  **Results and Analysis**:
    *   Examine the output images with detections saved in the `output/` directory.
    *   The script also prints the names, confidence scores, and bounding box coordinates of detected objects to the console.
    *   Understand the model's predictions, including the class of detected objects and their locations.
    *   The script discusses adjustable parameters like the choice of model and the confidence threshold.

## Data Annotation (Brief Overview)
While we are using a pre-trained model that already "knows" object classes and how to find them, training an object detection model from scratch (or fine-tuning one) requires a dataset with specific annotations. For each image, these annotations typically include:
- **Bounding Boxes**: Coordinates (e.g., x_min, y_min, x_max, y_max) defining a rectangle around each object of interest.
- **Class Labels**: The category of each object within a bounding box (e.g., 'car', 'person', 'dog').

This experiment leverages models pre-trained on datasets like COCO, which has 80 common object categories.

## Running Experiment 2

1.  **Prepare Environment**:
    *   Ensure PyTorch, Torchvision, and other required libraries (`requests`, `Pillow` (PIL), `numpy`) are installed. Refer to the "Setup" section for installation commands.
    *   An internet connection is required for the first run if:
        *   The pre-trained model weights need to be downloaded by PyTorch.
        *   The default sample image (`../data/sample_image.jpg`) is not present and needs to be downloaded from `DEFAULT_IMAGE_URL`.

2.  **Prepare Input Image**:
    *   The script will first look for an image named `sample_image.jpg` (or whatever `DEFAULT_IMAGE_FILENAME` is set to) in the `experiment2_object_detection/data/` directory.
    *   If this image is not found, the script will attempt to download the image specified by `DEFAULT_IMAGE_URL` (currently a picture of Zidane) and save it as `DEFAULT_IMAGE_FILENAME` in the `data/` directory.
    *   **To use your own image**:
        1. Place your image file (e.g., `my_photo.jpg`) into the `experiment2_object_detection/data/` directory.
        2. In `src/object_detector.py`, change the `DEFAULT_IMAGE_FILENAME` variable to your image's name (e.g., `DEFAULT_IMAGE_FILENAME = 'my_photo.jpg'`).

3.  **Run the Detection Script**:
    *   Navigate to the `experiment2_object_detection` directory in your terminal.
    *   Execute the `object_detector.py` script:
        \`\`\`bash
        cd experiment2_object_detection
        python src/object_detector.py
        \`\`\`
    *   The script will then:
        *   Load the specified image.
        *   Load the pre-trained Faster R-CNN model (downloading weights if it's the first time).
        *   Perform object detection.
        *   Print details of detected objects (class, score, box coordinates) to the console.
        *   Draw these detections on the image.
        *   Save the annotated image to the `experiment2_object_detection/output/` directory (e.g., `output/detected_sample_image.jpg`).

### Interpreting the Output

*   **Console Output**:
    *   **Setup Information**: PyTorch/Torchvision versions, device used (CUDA or CPU).
    *   **Image Status**: Messages about whether an existing image is used, or if a download is attempted/successful/failed.
    *   **Detections**: For each object found above the `CONFIDENCE_THRESHOLD`, the script prints:
        *   `Object: [class_name], Score: [confidence_score], Box: [x_min, y_min, x_max, y_max]`
*   **Visual Output**:
    *   An image file (e.g., `detected_sample_image.jpg`) will be saved in the `experiment2_object_detection/output/` directory. This image will have bounding boxes drawn around detected objects, along with their class labels and confidence scores.

### Adjustable Parts of the Script

You can modify `src/object_detector.py` to experiment with:
- **Different Models**:
    - Change `MODEL_WEIGHTS` and `MODEL_INSTANCE` to use other pre-trained object detection models available in `torchvision.models.detection` (e.g., different versions of Faster R-CNN, SSD, RetinaNet). Ensure you update `COCO_INSTANCE_CATEGORY_NAMES` if the new model uses a different set of categories or if `MODEL_WEIGHTS.meta["categories"]` provides them.
- **Confidence Threshold**:
    - Modify `CONFIDENCE_THRESHOLD` (e.g., from `0.5` to `0.7` for more confident detections, or `0.3` for more, possibly less accurate, detections).
- **Input Image**:
    - Change `DEFAULT_IMAGE_FILENAME` to the name of your image file (placed in the `data/` directory).
    - Change `DEFAULT_IMAGE_URL` if you want the script to download a different image when `DEFAULT_IMAGE_FILENAME` is not found.

### Troubleshooting and Notes
- **Internet Connection**: Required for the first run to download model weights from PyTorch and potentially the sample image. Subsequent runs with the same model and existing image may not need internet.
- **Dependencies**: Ensure `Pillow` (PIL) and `opencv-python` are installed, as they are commonly used for image manipulation in computer vision projects, although this script primarily uses Pillow for drawing. `requests` is needed for downloading the sample image. `numpy` is a general dependency.
- **Detection Speed**: Object detection can be computationally intensive. Running on a CPU (`device: cpu`) will be significantly slower than on a CUDA-enabled GPU (`device: cuda`).
- **Placeholder Image**: If the sample image download fails and you haven't placed an image manually, a grey placeholder image is created. The model will likely not detect any meaningful objects in this placeholder.
